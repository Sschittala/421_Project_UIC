{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mk2rfRjXpIns"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoTokenizer, AutoModel, get_scheduler\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_df = pd.read_csv(\"/content/trac2_CONVT_train.csv\", sep=',', engine='python', on_bad_lines='skip', escapechar='\\\\')\n",
        "dev_df = pd.read_csv(\"/content/trac2_CONVT_dev.csv\", sep=',', engine='python', on_bad_lines='skip', escapechar='\\\\')\n",
        "test_df = pd.read_csv(\"/content/trac2_CONVT_test.csv\", sep=',', engine='python', on_bad_lines='skip', escapechar='\\\\')"
      ],
      "metadata": {
        "id": "lPzytLFSpmp6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "def tokenize_texts(texts):\n",
        "  return tokenizer(\n",
        "      list(texts),\n",
        "      truncation=True,\n",
        "      padding=\"max_length\",\n",
        "      max_length=128,\n",
        "      return_tensors=\"pt\"\n",
        "  )\n",
        "train_tokens = tokenize_texts(train_df[\"text\"])\n",
        "dev_tokens = tokenize_texts(dev_df[\"text\"])\n",
        "test_tokens = tokenize_texts(test_df[\"text\"])\n",
        "for col in [\"Emotion\", \"EmotionalPolarity\"]:\n",
        "  train_df[col] = train_df[col].astype(int) - train_df[col].min()\n",
        "  dev_df[col] = dev_df[col].astype(int) - dev_df[col].min()\n",
        "train_emotion = torch.tensor(train_df[\"Emotion\"].values, dtype=torch.float)\n",
        "train_polarity = torch.tensor(train_df[\"EmotionalPolarity\"].values, dtype=torch.long)\n",
        "train_empathy = torch.tensor(train_df[\"Empathy\"].values, dtype=torch.float)\n",
        "dev_emotion = torch.tensor(dev_df[\"Emotion\"].values, dtype=torch.float)\n",
        "dev_emotional_polarity = torch.tensor(dev_df[\"Emotion\"].values, dtype=torch.long)\n",
        "dev_empathy = torch.tensor(dev_df[\"Emotion\"].values, dtype=torch.float)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DlDfWTh6qaxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f731c8-d20e-4aff-8ed6-852e3eb1e110"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskModel(nn.Module):\n",
        "  def __init__(self, model_name, polarities):\n",
        "    super().__init__()\n",
        "    self.bert = AutoModel.from_pretrained(model_name)\n",
        "    hidden_size = self.bert.config.hidden_size\n",
        "    dropout = self.bert.config.hidden_dropout_prob\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "    self.emotion_classifier = nn.Linear(hidden_size, 1)\n",
        "    self.polarity_classifier = nn.Linear(hidden_size, polarities)\n",
        "    self.empathy = nn.Linear(hidden_size, 1)\n",
        "  def forward(self, input_id, attention_mask):\n",
        "    outputs = self.bert(input_ids=input_id, attention_mask=attention_mask)\n",
        "    cls_output = self.drop(outputs.last_hidden_state[:, 0, :])\n",
        "    emotion = self.emotion_classifier(cls_output).squeeze(-1)\n",
        "    polarity = self.polarity_classifier(cls_output)\n",
        "    empathy = self.empathy(cls_output).squeeze(-1)\n",
        "    return emotion, polarity, empathy\n"
      ],
      "metadata": {
        "id": "JGrNHjetuB-_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(\n",
        "    train_tokens[\"input_ids\"], train_tokens[\"attention_mask\"], train_emotion, train_polarity, train_empathy\n",
        ")\n",
        "dev_dataset = TensorDataset(\n",
        "    dev_tokens[\"input_ids\"], dev_tokens[\"attention_mask\"], dev_emotion, dev_emotional_polarity, dev_empathy\n",
        ")"
      ],
      "metadata": {
        "id": "SaOVw-ftvYgE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "s-vtOGPrv1t0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_polar_labels = len(sorted(train_df[\"EmotionalPolarity\"].unique()))\n",
        "model = MultiTaskModel(model_name, num_polar_labels).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "num_epochs = 5\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ],
      "metadata": {
        "id": "PeTj7nQCv83F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epochs(model, dataloader, optimizer, scheduler):\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "  emotion_loss = nn.MSELoss()\n",
        "  polarity_loss = nn.CrossEntropyLoss()\n",
        "  empathy_loss = nn.MSELoss()\n",
        "  for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "    input_ids, attention_mask, emo_labl, emo_pol_lab, emp_lbl = [x.to(device) for x in batch]\n",
        "    optimizer.zero_grad()\n",
        "    emo_logits, emo_pol_logits, emp_logits = model(input_ids, attention_mask)\n",
        "    loss = emotion_loss(emo_logits, emo_labl) + polarity_loss(emo_pol_logits, emo_pol_lab) + empathy_loss(emp_logits, emp_lbl)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    total_loss += loss.item()\n",
        "  return total_loss / len(dataloader)\n"
      ],
      "metadata": {
        "id": "SaLIt1hexTka"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader):\n",
        "  model.eval()\n",
        "  emo_preds, emo_pol_preds, emp_preds = [], [], []\n",
        "  emo_lbls, emo_pol_lbls, emp_lbls = [], [], []\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "      input_ids, attention_mask, emo_labl, emo_pol_lab, emp_lbl = [x.to(device) for x in batch]\n",
        "      emo_logits, emo_pol_logits, emp_logits = model(input_ids, attention_mask)\n",
        "\n",
        "      emo_preds.extend(emo_logits.cpu().numpy().tolist())\n",
        "      emo_pol_preds.extend(torch.argmax(emo_pol_logits, dim=1).cpu().numpy().tolist())\n",
        "      emp_preds.extend(emp_logits.cpu().numpy().tolist())\n",
        "\n",
        "      emo_lbls.extend(emo_labl.cpu().numpy().tolist())\n",
        "      emo_pol_lbls.extend(emo_pol_lab.cpu().numpy().tolist())\n",
        "      emp_lbls.extend(emp_lbl.cpu().numpy().tolist())\n",
        "  return {\n",
        "      \"Emotion_MAE\": mean_absolute_error(emo_lbls, emo_preds),\n",
        "      \"Emotional_Polarity_Accuracy\": accuracy_score(emo_pol_lbls, emo_pol_preds),\n",
        "      \"Empathy_MAE\": mean_absolute_error(emp_lbls, emp_preds),\n",
        "      \"Polarity_F1\": f1_score(emo_pol_lbls, emo_pol_preds, average=\"weighted\")\n",
        "\n",
        "  }"
      ],
      "metadata": {
        "id": "F0QQv8OyyZaW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  train_loss = train_epochs(model, train_loader, optimizer, lr_scheduler)\n",
        "  metrics = evaluate(model, dev_loader)\n",
        "  print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "  print(f\"Train Loss: {train_loss:.4f}\")\n",
        "  print(f\"Emotion MAE: {metrics['Emotion_MAE']:.4f}\")\n",
        "  print(f\"Empathy MAE: {metrics['Empathy_MAE']:.4f}\")\n",
        "  print(f\"Emotional Polarity Accuracy: {metrics['Emotional_Polarity_Accuracy']:.4f}, F1: {metrics['Polarity_F1']:.4f}\")\n",
        "  print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "6grEuk5Azqj8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f951adb-e2f7-42fb-ee03-14fc53f823c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Train Loss: 1.9602\n",
            "Emotion MAE: 0.4976\n",
            "Empathy MAE: 0.5512\n",
            "Emotional Polarity Accuracy: 0.4071, F1: 0.3526\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/5\n",
            "Train Loss: 1.4755\n",
            "Emotion MAE: 0.5016\n",
            "Empathy MAE: 0.5726\n",
            "Emotional Polarity Accuracy: 0.3859, F1: 0.3369\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/5\n",
            "Train Loss: 1.3187\n",
            "Emotion MAE: 0.4844\n",
            "Empathy MAE: 0.5663\n",
            "Emotional Polarity Accuracy: 0.3990, F1: 0.3521\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/5\n",
            "Train Loss: 1.2122\n",
            "Emotion MAE: 0.5015\n",
            "Empathy MAE: 0.5937\n",
            "Emotional Polarity Accuracy: 0.3960, F1: 0.3515\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/5\n",
            "Train Loss: 1.1368\n",
            "Emotion MAE: 0.5020\n",
            "Empathy MAE: 0.6054\n",
            "Emotional Polarity Accuracy: 0.3808, F1: 0.3358\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "emo_preds, emo_pol_preds, emp_preds = [], [], []\n",
        "test_ids = test_df[\"id\"].tolist()\n",
        "input_ids, mask = test_tokens[\"input_ids\"], test_tokens[\"attention_mask\"]\n",
        "with torch.no_grad():\n",
        "  for i in range(0, len(input_ids), 16):\n",
        "    batch_ids = input_ids[i:i+16].to(device)\n",
        "    batch_mask = mask[i:i+16].to(device)\n",
        "    emo_logits, emo_pol_logits, emp_logits = model(batch_ids, batch_mask)\n",
        "    emo_preds.extend(emo_logits.cpu().numpy().tolist())\n",
        "    emo_pol_preds.extend(torch.argmax(emo_pol_logits, dim=1).cpu().numpy().tolist())\n",
        "    emp_preds.extend(emp_logits.cpu().numpy().tolist())\n",
        "pred_df = pd.DataFrame({\n",
        "    \"id\": test_ids,\n",
        "    \"Emotion\": emo_preds,\n",
        "    \"EmotionalPolarity\": emo_pol_preds,\n",
        "    \"Empathy\": emp_preds\n",
        "})\n",
        "pred_df.to_csv(\"predictions_bert.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "7VmecUm8_iip"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}